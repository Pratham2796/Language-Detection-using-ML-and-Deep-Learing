{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Required Libraries\n",
    "import pandas as pd  \n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Text Preprocessing\n",
    "import nltk\n",
    "#nltk.download(\"all\")   # you will need to download it if you have not done so\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from lightgbm import LGBMClassifier\n",
    "import keras\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv('Language.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Estonian', 'Swedish', 'Thai', 'Tamil', 'Dutch', 'Japanese',\n",
       "       'Turkish', 'Latin', 'Urdu', 'Indonesian', 'Portugese', 'French',\n",
       "       'Chinese', 'Korean', 'Hindi', 'Spanish', 'Pushto', 'Persian',\n",
       "       'Romanian', 'Russian', 'English', 'Arabic'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['language'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "English_text = df[df['language']=='English']['Text'].apply(stemmer)\n",
    "English_text = English_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"swedish\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('swedish')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Swedish_text = df[df['language']=='Swedish']['Text'].apply(stemmer)\n",
    "Swedish_text = Swedish_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"dutch\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('dutch')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Dutch_text = df[df['language']=='Dutch']['Text'].apply(stemmer)\n",
    "Dutch_text = Dutch_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    " df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"french\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('french')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "French_text = df[df['language']=='French']['Text'].apply(stemmer)\n",
    "French_text = French_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"spanish\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('spanish')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Spanish_text = df[df['language']=='Spanish']['Text'].apply(stemmer)\n",
    "Spanish_text = Spanish_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"romanian\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('romanian')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Romanian_text = df[df['language']=='Romanian']['Text'].apply(stemmer)\n",
    "Romanian_text = Romanian_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"russian\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('russian')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Russian_text = df[df['language']=='Russian']['Text'].apply(stemmer)\n",
    "Russian_text = Russian_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer(text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"arabic\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words\n",
    "\n",
    "def puncStopW(text):\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('arabic')]\n",
    "    \n",
    "    return \" \".join(text)\n",
    "\n",
    "Arabic_text = df[df['language']=='Arabic']['Text'].apply(stemmer)\n",
    "Arabic_text = Arabic_text.apply(puncStopW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new=pd.DataFrame(columns=['Text','language'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages=['Arabic','Russian','Romanian','Spanish','French','Dutch','Swedish','English']\n",
    "temp=[Arabic_text,Russian_text,Romanian_text,Spanish_text,French_text,Dutch_text,Swedish_text,English_text]\n",
    "for i in range(0,8):\n",
    "    for x in temp[i]:\n",
    "        data_new = data_new.append({'Text': x,'language':languages[i]},ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 2)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>عام ضبط تاريخ اعل بغداد خط هدم اسوار تاريخ هاج...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>عندم وصل جنود حمل فرنس غرب مدين اسكندر يول عام...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>بصو الشج مميز خطيب مميز صاحب مدرس طور قل تجد م...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ليل دخل مسلح داعش مدين رماد عاصم محافظ انبار و...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>درج خط طول مقسم الى دقيق ينقسم الى يتم كتاب خط...</td>\n",
       "      <td>Arabic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>march empti mirror press publish epstein polit...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>musk want go mar back human worthi goal lot em...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>overal male black abov white also white superc...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>tim reynold born decemb wiesbaden germani gram...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>total high school popul approach committe conc...</td>\n",
       "      <td>English</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text language\n",
       "0     عام ضبط تاريخ اعل بغداد خط هدم اسوار تاريخ هاج...   Arabic\n",
       "1     عندم وصل جنود حمل فرنس غرب مدين اسكندر يول عام...   Arabic\n",
       "2     بصو الشج مميز خطيب مميز صاحب مدرس طور قل تجد م...   Arabic\n",
       "3     ليل دخل مسلح داعش مدين رماد عاصم محافظ انبار و...   Arabic\n",
       "4     درج خط طول مقسم الى دقيق ينقسم الى يتم كتاب خط...   Arabic\n",
       "...                                                 ...      ...\n",
       "7995  march empti mirror press publish epstein polit...  English\n",
       "7996  musk want go mar back human worthi goal lot em...  English\n",
       "7997  overal male black abov white also white superc...  English\n",
       "7998  tim reynold born decemb wiesbaden germani gram...  English\n",
       "7999  total high school popul approach committe conc...  English\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "data_new = shuffle(data_new)\n",
    "data_new = shuffle(data_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transforming a text into a vector on the basis of the frequency (count) of each word that occurs in the entire text.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vec = CountVectorizer()\n",
    "vec.fit(data_new[\"Text\"])\n",
    "features_np = vec.transform(data_new[\"Text\"]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y=le.fit_transform(data_new['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Partition for Traditional Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting data into 70% training data and 30% testing data using stratify\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_np,y,\n",
    "                                          test_size=0.3, random_state=1,\n",
    "                                            stratify=data_new['language'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using MultinomialNB is 99.5%\n",
      "The confusion matrix is:\n",
      "[[298   1   1   0   0   0   0   0]\n",
      " [  1 298   0   1   0   0   0   0]\n",
      " [  0   0 299   0   0   0   0   1]\n",
      " [  0   0   1 297   1   0   0   1]\n",
      " [  0   1   0   0 299   0   0   0]\n",
      " [  0   0   0   1   1 297   0   1]\n",
      " [  0   0   0   0   0   0 300   0]\n",
      " [  0   0   0   0   0   0   0 300]]\n",
      "The precision score is:\n",
      "0.9950137327341774\n",
      "The recall score is:\n",
      "0.995\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_hat=clf.predict(X_test)\n",
    "a=accuracy_score(y_test,y_hat)\n",
    "print('The accuracy score using MultinomialNB is {}%'.format(a*100))\n",
    "print(\"The confusion matrix is:\")\n",
    "print(confusion_matrix(y_test, y_hat))\n",
    "print(\"The precision score is:\")\n",
    "print(precision_score(y_test, y_hat, average='weighted'))\n",
    "print(\"The recall score is:\")\n",
    "print(recall_score(y_test, y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Arabic'], dtype='<U8')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=['صباح الخير.']\n",
    "test_input = vec.transform(test_data).toarray()\n",
    "y_hat=clf.predict(test_input)\n",
    "predictions = encoder.inverse_transform(y_hat)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using Logistic is 98.95833333333334%\n",
      "The confusion matrix is:\n",
      "[[299   0   0   0   1   0   0   0]\n",
      " [  0 295   2   0   0   0   3   0]\n",
      " [  0   0 299   0   0   0   0   1]\n",
      " [  0   2   0 295   1   0   2   0]\n",
      " [  0   1   2   2 293   0   2   0]\n",
      " [  0   0   0   1   0 297   2   0]\n",
      " [  0   0   1   1   0   0 298   0]\n",
      " [  0   0   0   0   1   0   0 299]]\n",
      "The precision score is:\n",
      "0.9896653952103827\n",
      "The recall score is:\n",
      "0.9895833333333334\n"
     ]
    }
   ],
   "source": [
    "#Applying LogisticRegression classifier\n",
    "logistic_model = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_hat=logistic_model.predict(X_test)\n",
    "a=accuracy_score(y_test,y_hat)\n",
    "print('The accuracy score using Logistic is {}%'.format(a*100))\n",
    "print(\"The confusion matrix is:\")\n",
    "print(confusion_matrix(y_test, y_hat))\n",
    "print(\"The precision score is:\")\n",
    "print(precision_score(y_test, y_hat,average='weighted'))\n",
    "print(\"The recall score is:\")\n",
    "print(recall_score(y_test, y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Spanish'], dtype='<U8')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=['¿Cómo te llamas ']\n",
    "test_input = vec.transform(test_data).toarray()\n",
    "y_hat=logistic_model.predict(test_input)\n",
    "predictions = encoder.inverse_transform(y_hat)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:30:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "The accuracy score using KNeighborsClassifier is 95.875%\n",
      "The confusion matrix is:\n",
      "[[294   1   4   0   1   0   0   0]\n",
      " [  0 283  12   1   0   0   4   0]\n",
      " [  0   0 293   5   1   0   0   1]\n",
      " [  0   3   9 282   0   0   4   2]\n",
      " [  0   1   8   1 284   0   6   0]\n",
      " [  0   0   9   1   0 288   2   0]\n",
      " [  0   0   8   4   1   0 287   0]\n",
      " [  0   1   6   1   1   0   1 290]]\n",
      "The precision score is:\n",
      "0.9618329569528411\n",
      "The recall score is:\n",
      "0.95875\n"
     ]
    }
   ],
   "source": [
    "#Applying XGBoost classifier\n",
    "xgbc = XGBClassifier()\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_hat=xgbc.predict(X_test)\n",
    "a=accuracy_score(y_test,y_hat)\n",
    "print('The accuracy score using KNeighborsClassifier is {}%'.format(a*100))\n",
    "print(\"The confusion matrix is:\")\n",
    "print(confusion_matrix(y_test, y_hat))\n",
    "print(\"The precision score is:\")\n",
    "print(precision_score(y_test, y_hat, average='weighted'))\n",
    "print(\"The recall score is:\")\n",
    "print(recall_score(y_test, y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['English'], dtype='<U8')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data=['How are you? ']\n",
    "test_input = vec.transform(test_data).toarray()\n",
    "y_hat=xgbc.predict(test_input)\n",
    "predictions = encoder.inverse_transform(y_hat)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using RandomForestClassifier_model is 93.16666666666666%\n",
      "The confusion matrix is:\n",
      "[[289   0  11   0   0   0   0   0]\n",
      " [  0 267  28   1   0   0   1   3]\n",
      " [  0   1 291   3   4   0   0   1]\n",
      " [  0   0  15 280   2   0   3   0]\n",
      " [  0   0  10   1 285   0   2   2]\n",
      " [  0   0  13   1   1 284   1   0]\n",
      " [  0   0  19   3   1   0 277   0]\n",
      " [  0   3  28   3   1   0   2 263]]\n",
      "The precision score is:\n",
      "0.9451203965230536\n",
      "The recall score is:\n",
      "0.9316666666666666\n"
     ]
    }
   ],
   "source": [
    "#Applying RandomForestClassifier classifier\n",
    "RandomForestClassifier_model = RandomForestClassifier(max_depth=10, random_state=0)\n",
    "RandomForestClassifier_model.fit(X_train, y_train)\n",
    "y_hat=RandomForestClassifier_model.predict(X_test)\n",
    "a=accuracy_score(y_test,y_hat)\n",
    "print('The accuracy score using RandomForestClassifier_model is {}%'.format(a*100))\n",
    "print(\"The confusion matrix is:\")\n",
    "print(confusion_matrix(y_test, y_hat))\n",
    "print(\"The precision score is:\")\n",
    "print(precision_score(y_test, y_hat, average='weighted'))\n",
    "print(\"The recall score is:\")\n",
    "print(recall_score(y_test, y_hat, average='weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score using KNeighborsClassifier is 52.79166666666667%\n",
      "The confusion matrix is:\n",
      "[[118   0   0 182   0   0   0   0]\n",
      " [  0 135   0 163   2   0   0   0]\n",
      " [  0   0 129 171   0   0   0   0]\n",
      " [  0   1   0 295   1   1   2   0]\n",
      " [  0   0   0 147 151   1   1   0]\n",
      " [  0   0   0 174   0 126   0   0]\n",
      " [  0   0   0 195   1   0 104   0]\n",
      " [  0   1   1  88   1   0   0 209]]\n",
      "The precision score is:\n",
      "0.888809506537985\n",
      "The recall score is:\n",
      "0.5279166666666667\n",
      "The accuracy score using KNeighborsClassifier is 53.625%\n",
      "The confusion matrix is:\n",
      "[[158   0   0 142   0   0   0   0]\n",
      " [  0 204   1  95   0   0   0   0]\n",
      " [  0   3 192 105   0   0   0   0]\n",
      " [  0   2   5 290   1   1   1   0]\n",
      " [  0   1   1 189 109   0   0   0]\n",
      " [  0   0   0 219   0  81   0   0]\n",
      " [  0   2   0 235   1   0  62   0]\n",
      " [  0   2   1 104   1   0   1 191]]\n",
      "The precision score is:\n",
      "0.8816671885870939\n",
      "The recall score is:\n",
      "0.53625\n",
      "The accuracy score using KNeighborsClassifier is 60.70833333333333%\n",
      "The confusion matrix is:\n",
      "[[119   0   0 181   0   0   0   0]\n",
      " [  0 181   1 118   0   0   0   0]\n",
      " [  0   6 132 161   0   0   1   0]\n",
      " [  0   0   2 295   1   1   1   0]\n",
      " [  0   1   5 124 170   0   0   0]\n",
      " [  0   0   0 102   0 198   0   0]\n",
      " [  0   4   5 142   0   0 149   0]\n",
      " [  0   2   2  81   1   0   1 213]]\n",
      "The precision score is:\n",
      "0.8799469513725815\n",
      "The recall score is:\n",
      "0.6070833333333333\n",
      "The accuracy score using KNeighborsClassifier is 56.458333333333336%\n",
      "The confusion matrix is:\n",
      "[[111   0   0 189   0   0   0   0]\n",
      " [  0 172   1 125   2   0   0   0]\n",
      " [  0   2 127 169   0   0   2   0]\n",
      " [  0   1   0 296   1   1   1   0]\n",
      " [  0   0   1 139 159   0   1   0]\n",
      " [  0   0   0 142   0 158   0   0]\n",
      " [  0   4   2 167   0   0 126   1]\n",
      " [  0   3   0  89   1   0   1 206]]\n",
      "The precision score is:\n",
      "0.8832020704731942\n",
      "The recall score is:\n",
      "0.5645833333333333\n",
      "The accuracy score using KNeighborsClassifier is 57.375%\n",
      "The confusion matrix is:\n",
      "[[ 95   0   0 205   0   0   0   0]\n",
      " [  0 170   0 130   0   0   0   0]\n",
      " [  0   2 124 172   0   0   1   1]\n",
      " [  0   1   0 295   1   1   2   0]\n",
      " [  0   0   0 139 161   0   0   0]\n",
      " [  0   0   0 113   0 187   0   0]\n",
      " [  0   2   0 158   1   0 138   1]\n",
      " [  0   1   2  89   1   0   0 207]]\n",
      "The precision score is:\n",
      "0.8902908655398531\n",
      "The recall score is:\n",
      "0.57375\n",
      "The accuracy score using KNeighborsClassifier is 53.87499999999999%\n",
      "The confusion matrix is:\n",
      "[[ 81   0   0 219   0   0   0   0]\n",
      " [  0 162   0 138   0   0   0   0]\n",
      " [  0   3 104 192   0   0   1   0]\n",
      " [  0   0   0 296   1   1   2   0]\n",
      " [  0   0   0 132 167   0   0   1]\n",
      " [  0   0   0 151   0 149   0   0]\n",
      " [  0   3   0 168   1   0 128   0]\n",
      " [  0   1   1  90   1   0   1 206]]\n",
      "The precision score is:\n",
      "0.8878965765036982\n",
      "The recall score is:\n",
      "0.53875\n",
      "The accuracy score using KNeighborsClassifier is 56.291666666666664%\n",
      "The confusion matrix is:\n",
      "[[ 84   0   0 216   0   0   0   0]\n",
      " [  0 188   0 111   0   0   0   1]\n",
      " [  0   2 107 190   0   0   1   0]\n",
      " [  0   0   0 297   0   1   2   0]\n",
      " [  0   0   0 137 162   0   1   0]\n",
      " [  0   0   0 109   0 191   0   0]\n",
      " [  0   2   1 174   1   0 121   1]\n",
      " [  0   1   1  95   0   0   2 201]]\n",
      "The precision score is:\n",
      "0.8888476655585559\n",
      "The recall score is:\n",
      "0.5629166666666666\n",
      "The accuracy score using KNeighborsClassifier is 57.54166666666667%\n",
      "The confusion matrix is:\n",
      "[[ 83   0   0 217   0   0   0   0]\n",
      " [  0 187   0 113   0   0   0   0]\n",
      " [  0   1 113 185   0   0   1   0]\n",
      " [  0   0   0 297   0   1   2   0]\n",
      " [  0   0   0 135 165   0   0   0]\n",
      " [  0   0   0 100   0 200   0   0]\n",
      " [  0   3   0 161   1   0 134   1]\n",
      " [  0   2   0  94   1   0   1 202]]\n",
      "The precision score is:\n",
      "0.8932699660342155\n",
      "The recall score is:\n",
      "0.5754166666666667\n",
      "The accuracy score using KNeighborsClassifier is 57.666666666666664%\n",
      "The confusion matrix is:\n",
      "[[ 80   0   0 220   0   0   0   0]\n",
      " [  0 206   0  94   0   0   0   0]\n",
      " [  0   1 111 187   0   0   1   0]\n",
      " [  0   0   0 298   0   0   2   0]\n",
      " [  0   0   0 136 164   0   0   0]\n",
      " [  0   0   0 113   0 187   0   0]\n",
      " [  0   2   0 164   1   0 132   1]\n",
      " [  0   2   0  90   1   0   1 206]]\n",
      "The precision score is:\n",
      "0.8948613863022085\n",
      "The recall score is:\n",
      "0.5766666666666667\n",
      "The accuracy score using KNeighborsClassifier is 59.62499999999999%\n",
      "The confusion matrix is:\n",
      "[[ 73   0   0 227   0   0   0   0]\n",
      " [  0 225   0  75   0   0   0   0]\n",
      " [  0   1 109 189   0   0   0   1]\n",
      " [  0   0   0 297   1   0   1   1]\n",
      " [  0   0   0 133 167   0   0   0]\n",
      " [  0   0   0  74   0 226   0   0]\n",
      " [  0   1   0 171   1   0 126   1]\n",
      " [  0   2   0  88   0   0   2 208]]\n",
      "The precision score is:\n",
      "0.896258339174509\n",
      "The recall score is:\n",
      "0.59625\n",
      "The accuracy score using KNeighborsClassifier is 60.458333333333336%\n",
      "The confusion matrix is:\n",
      "[[ 73   0   0 227   0   0   0   0]\n",
      " [  0 235   0  65   0   0   0   0]\n",
      " [  0   1 112 186   0   0   1   0]\n",
      " [  0   0   0 298   0   0   1   1]\n",
      " [  0   0   0 132 167   0   1   0]\n",
      " [  0   0   0  74   0 226   0   0]\n",
      " [  0   2   0 163   1   0 133   1]\n",
      " [  0   3   0  88   1   0   1 207]]\n",
      "The precision score is:\n",
      "0.8957737373809593\n",
      "The recall score is:\n",
      "0.6045833333333334\n",
      "The accuracy score using KNeighborsClassifier is 61.5%\n",
      "The confusion matrix is:\n",
      "[[ 72   0   0 228   0   0   0   0]\n",
      " [  0 236   0  64   0   0   0   0]\n",
      " [  0   2 113 184   0   0   1   0]\n",
      " [  0   1   0 296   1   0   1   1]\n",
      " [  0   0   0 125 175   0   0   0]\n",
      " [  0   0   0  67   0 233   0   0]\n",
      " [  0   2   0 156   1   0 140   1]\n",
      " [  0   5   0  82   0   0   2 211]]\n",
      "The precision score is:\n",
      "0.894642368616134\n",
      "The recall score is:\n",
      "0.615\n",
      "The accuracy score using KNeighborsClassifier is 61.75000000000001%\n",
      "The confusion matrix is:\n",
      "[[ 76   0   0 224   0   0   0   0]\n",
      " [  0 231   0  69   0   0   0   0]\n",
      " [  0   1 106 192   0   0   1   0]\n",
      " [  0   0   0 297   1   1   0   1]\n",
      " [  0   1   0 121 178   0   0   0]\n",
      " [  0   0   0  62   0 238   0   0]\n",
      " [  0   3   1 151   1   0 143   1]\n",
      " [  0   3   0  82   1   0   1 213]]\n",
      "The precision score is:\n",
      "0.8951550594780627\n",
      "The recall score is:\n",
      "0.6175\n",
      "The accuracy score using KNeighborsClassifier is 62.958333333333336%\n",
      "The confusion matrix is:\n",
      "[[ 72   0   0 228   0   0   0   0]\n",
      " [  0 234   0  66   0   0   0   0]\n",
      " [  0   2 111 186   0   0   1   0]\n",
      " [  0   0   0 298   1   0   0   1]\n",
      " [  0   1   0 117 181   0   1   0]\n",
      " [  0   0   0  53   0 247   0   0]\n",
      " [  0   4   1 139   1   0 154   1]\n",
      " [  0   6   0  79   1   0   0 214]]\n",
      "The precision score is:\n",
      "0.8944537929729226\n",
      "The recall score is:\n",
      "0.6295833333333334\n",
      "The accuracy score using KNeighborsClassifier is 63.95833333333333%\n",
      "The confusion matrix is:\n",
      "[[ 74   0   0 211   0  15   0   0]\n",
      " [  0 242   0  58   0   0   0   0]\n",
      " [  0   4 110 184   0   1   1   0]\n",
      " [  0   2   0 295   0   1   1   1]\n",
      " [  0   2   0 106 191   1   0   0]\n",
      " [  0   0   0  56   0 244   0   0]\n",
      " [  0   2   0 133   1   0 163   1]\n",
      " [  0   9   0  73   1   0   1 216]]\n",
      "The precision score is:\n",
      "0.8856535496545922\n",
      "The recall score is:\n",
      "0.6395833333333333\n",
      "The accuracy score using KNeighborsClassifier is 64.33333333333333%\n",
      "The confusion matrix is:\n",
      "[[ 67   0   0 212   0  21   0   0]\n",
      " [  0 238   0  61   0   1   0   0]\n",
      " [  0   2 115 177   0   4   0   2]\n",
      " [  0   1   0 292   0   5   1   1]\n",
      " [  0   3   0 103 191   2   1   0]\n",
      " [  0   0   0  48   0 252   0   0]\n",
      " [  0   4   0 122   1   0 172   1]\n",
      " [  0   9   0  71   0   3   0 217]]\n",
      "The precision score is:\n",
      "0.8793930645762932\n",
      "The recall score is:\n",
      "0.6433333333333333\n",
      "The accuracy score using KNeighborsClassifier is 65.04166666666666%\n",
      "The confusion matrix is:\n",
      "[[ 69   0   0 196   0  34   1   0]\n",
      " [  0 240   0  57   0   3   0   0]\n",
      " [  0   3 118 167   0  10   1   1]\n",
      " [  0   2   0 291   0   5   1   1]\n",
      " [  0   1   0 106 191   0   1   1]\n",
      " [  0   0   0  46   0 253   1   0]\n",
      " [  0   6   0 115   1   2 175   1]\n",
      " [  0  11   0  58   0   7   0 224]]\n",
      "The precision score is:\n",
      "0.8685797590901675\n",
      "The recall score is:\n",
      "0.6504166666666666\n",
      "The accuracy score using KNeighborsClassifier is 65.45833333333333%\n",
      "The confusion matrix is:\n",
      "[[ 70   0   0 187   0  42   1   0]\n",
      " [  0 239   0  55   0   6   0   0]\n",
      " [  0   2 111 170   0  14   1   2]\n",
      " [  0   3   0 283   0   9   4   1]\n",
      " [  0   1   0 103 194   2   0   0]\n",
      " [  0   0   0  42   0 257   1   0]\n",
      " [  0   5   0  95   1   7 191   1]\n",
      " [  0   7   0  54   1  10   2 226]]\n",
      "The precision score is:\n",
      "0.8605184166424826\n",
      "The recall score is:\n",
      "0.6545833333333333\n",
      "The accuracy score using KNeighborsClassifier is 67.25%\n",
      "The confusion matrix is:\n",
      "[[ 70   0   0 183   0  43   4   0]\n",
      " [  0 242   0  49   0   9   0   0]\n",
      " [  0   5 118 151   0  17   6   3]\n",
      " [  0   2   0 288   0   7   2   1]\n",
      " [  0   4   0  81 206   6   3   0]\n",
      " [  0   0   0  37   0 263   0   0]\n",
      " [  0   5   0  82   1   9 202   1]\n",
      " [  0   8   0  59   1   6   1 225]]\n",
      "The precision score is:\n",
      "0.8556573000672012\n",
      "The recall score is:\n",
      "0.6725\n"
     ]
    }
   ],
   "source": [
    " for i in range(1,30):\n",
    "        neigh = KNeighborsClassifier(n_neighbors=i)\n",
    "        neigh.fit(X_train, y_train)\n",
    "        y_hat=neigh.predict(X_test)\n",
    "        a=accuracy_score(y_test,y_hat)\n",
    "        print('The accuracy score using KNeighborsClassifier is {}%'.format(a*100))\n",
    "        print(\"The confusion matrix is:\")\n",
    "        print(confusion_matrix(y_test, y_hat))\n",
    "        print(\"The precision score is:\")\n",
    "        print(precision_score(y_test, y_hat, average='weighted'))\n",
    "        print(\"The recall score is:\")\n",
    "        print(recall_score(y_test, y_hat, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing for ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "#Fit encoder\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['Arabic','Russian','Romanian','Spanish','French','Dutch','Swedish','English'])\n",
    "y_encoded = encoder.transform(data_new['language'])\n",
    "y_dummy = np_utils.to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_np,y_dummy,\n",
    "                                          test_size=0.3, random_state=1,stratify=y_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(500,input_dim=X_train.shape[1],activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100,activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8,activation=\"softmax\"))\n",
    "model_optimizer = tf.keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "88/88 - 15s - loss: 2.1069 - accuracy: 0.1918 - val_loss: 1.8707 - val_accuracy: 0.3125 - 15s/epoch - 172ms/step\n",
      "Epoch 2/10\n",
      "88/88 - 12s - loss: 1.5430 - accuracy: 0.4545 - val_loss: 0.8748 - val_accuracy: 0.9696 - 12s/epoch - 139ms/step\n",
      "Epoch 3/10\n",
      "88/88 - 11s - loss: 0.6466 - accuracy: 0.8431 - val_loss: 0.1808 - val_accuracy: 0.9984 - 11s/epoch - 127ms/step\n",
      "Epoch 4/10\n",
      "88/88 - 12s - loss: 0.2263 - accuracy: 0.9665 - val_loss: 0.0668 - val_accuracy: 1.0000 - 12s/epoch - 134ms/step\n",
      "Epoch 5/10\n",
      "88/88 - 11s - loss: 0.1140 - accuracy: 0.9868 - val_loss: 0.0317 - val_accuracy: 1.0000 - 11s/epoch - 126ms/step\n",
      "Epoch 6/10\n",
      "88/88 - 12s - loss: 0.0643 - accuracy: 0.9932 - val_loss: 0.0198 - val_accuracy: 1.0000 - 12s/epoch - 132ms/step\n",
      "Epoch 7/10\n",
      "88/88 - 11s - loss: 0.0469 - accuracy: 0.9954 - val_loss: 0.0142 - val_accuracy: 1.0000 - 11s/epoch - 128ms/step\n",
      "Epoch 8/10\n",
      "88/88 - 11s - loss: 0.0341 - accuracy: 0.9984 - val_loss: 0.0094 - val_accuracy: 1.0000 - 11s/epoch - 127ms/step\n",
      "Epoch 9/10\n",
      "88/88 - 11s - loss: 0.0251 - accuracy: 0.9988 - val_loss: 0.0075 - val_accuracy: 1.0000 - 11s/epoch - 127ms/step\n",
      "Epoch 10/10\n",
      "88/88 - 11s - loss: 0.0184 - accuracy: 0.9991 - val_loss: 0.0068 - val_accuracy: 1.0000 - 11s/epoch - 124ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train,\n",
    "          validation_split=0.10,epochs=10,\n",
    "          batch_size=64,\n",
    "          verbose=2,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=['I am good']\n",
    "test_input = vec.transform(test_data).toarray()\n",
    "labels = np.argmax(model.predict(test_input), axis=-1) #model.predict(X_test)\n",
    "predictions = encoder.inverse_transform(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 63105)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Dutch'], dtype='<U8')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(round(train_acc[1]*100,2))\n",
    "print(round(val_acc[1]*100,2))\n",
    "print('The training accuracy for AN is {}%'.format(round(train_acc[1]*100,2)))\n",
    "print('The testing accuracy for AN is {}%'.format(round(val_acc[1]*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review_length = 500\n",
    "top_words = 5000\n",
    "embedding_vecor_length = 32\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(1000, embedding_vecor_length, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(2, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train, y_train)\n",
    "val_acc = model.evaluate(X_test, y_test)\n",
    "print('The training accuracy for N-Grams is {}%'.format(round(train_acc[1]*100,2)))\n",
    "print('The testing accuracy for N-Grams is {}%'.format(round(val_acc[1]*100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing for N-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trigrams(corpus,n_feat=200):\n",
    "    \"\"\"\n",
    "    Returns a list of the N most common character trigrams from a list of sentences\n",
    "    params\n",
    "    ------------\n",
    "        corpus: list of strings\n",
    "        n_feat: integer\n",
    "    \"\"\"\n",
    "    \n",
    "    #fit the n-gram model\n",
    "    vectorizer = CountVectorizer(analyzer='char',\n",
    "                            ngram_range=(4, 4)\n",
    "                            ,max_features=n_feat)\n",
    "    \n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    #Get model feature names\n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    return feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtain trigrams from each language\n",
    "features = {}\n",
    "features_set = set()\n",
    "\n",
    "for l in languages:\n",
    "    \n",
    "    #get corpus filtered by language\n",
    "    corpus = data_new[data_new.language==l]['Text']\n",
    "    \n",
    "    #get 200 most frequent trigrams\n",
    "    trigrams = get_trigrams(corpus)\n",
    "    \n",
    "    #add to dict and set\n",
    "    features[l] = trigrams \n",
    "    features_set.update(trigrams)\n",
    "\n",
    "    \n",
    "#create vocabulary list using feature set\n",
    "vocab = dict()\n",
    "for i,f in enumerate(features_set):\n",
    "    vocab[f]=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train count vectoriser using vocabulary\n",
    "vectorizer = CountVectorizer(analyzer='char',\n",
    "                             ngram_range=(4, 4),\n",
    "                            vocabulary=vocab)\n",
    "\n",
    "#create feature matrix for training set\n",
    "corpus = data_new['Text']   \n",
    "X = vectorizer.fit_transform(corpus)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "features_np = pd.DataFrame(data=X.toarray(),columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(['Arabic','Russian','Romanian','Spanish','French','Dutch','Swedish','English'])\n",
    "y_encoded = encoder.transform(data_new['language'])\n",
    "y_dummy = np_utils.to_categorical(y_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features_np,y_dummy,\n",
    "                                          test_size=0.15,stratify=y_dummy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of N-Gram DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='sigmoid'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=64) #fit ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = model.evaluate(X_train, y_train)\n",
    "val_acc = model.evaluate(X_test, y_test)\n",
    "print('The training accuracy for N-Grams is {}%'.format(round(train_acc[1]*100,2)))\n",
    "print('The testing accuracy for N-Grams is {}%'.format(round(val_acc[1]*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
